#!/bin/bash

echo
echo "ğŸ› ï¸  [STEP 1] Updating packages..."
sudo apt update && sudo apt upgrade -y

echo
echo "ğŸ“¦ [STEP 2] Installing dependencies..."
sudo apt install -y git cmake clang build-essential aria2

echo
echo "ğŸ“ [STEP 3] Creating helper folder at ~/llm..."
mkdir -p ~/llm
cd ~/llm || { echo "âŒ Failed to create ~/llm"; exit 1; }

echo
echo "ğŸ”§ [STEP 4] Creating ram_fix.sh..."
cat <<'EOF' > ram_fix.sh
#!/bin/bash
cd ~/llama.cpp || exit
make LLAMA_CUBLAS=0 LLAMA_METAL=0 -j4
EOF
chmod +x ram_fix.sh

echo
echo "â¬‡ï¸  [STEP 5] Creating install_mistral.sh (model download helper)..."
cat <<'EOF' > install_mistral.sh
#!/bin/bash
mkdir -p ~/llama.cpp/models
cd ~/llama.cpp/models || exit
aria2c -x 16 "https://huggingface.co/TheBloke/Mistral-7B-GGUF/resolve/main/mistral-7b.Q4_K_M.gguf"
EOF
chmod +x install_mistral.sh

echo
echo "ğŸŒ [STEP 6] Cloning llama.cpp..."
cd ~
git clone https://github.com/ggerganov/llama.cpp || { echo "âŒ Git clone failed! Check your internet."; exit 1; }

echo
echo "âœ… Setup completed successfully!"
echo
echo "ğŸš€ NEXT STEPS:"
echo "1ï¸âƒ£  Build llama.cpp with:"
echo "    bash ~/llm/ram_fix.sh"
echo
echo "2ï¸âƒ£  Download Mistral model (Q4) with:"
echo "    bash ~/llm/install_mistral.sh"
echo
echo "3ï¸âƒ£  Run the model with:"
echo '    ./main -m ./models/mistral-7b.Q4_K_M.gguf -p "Hello!"'
echo
echo "ğŸ’¬  Or run in interactive chat mode:"
echo '    ./main -m ./models/mistral-7b.Q4_K_M.gguf -i'
echo
echo "ğŸ“  llama.cpp is at: ~/llama.cpp"
echo "ğŸ› ï¸  Helper scripts at: ~/llm/"
echo
